# Project 3: Classification & Clustering

ç”µ02 è‚–é”¦æ¾ 2020010563

## Getting Started

run `python dataClassifier.py â€“c lr`

è¯¥å‘½ä»¤è°ƒç”¨Linear Regression Classifierï¼Œè¿è¡Œç»“æœå¦‚ä¸‹ï¼Œå‡†ç¡®ç‡ä¸º77.9%. è¯¥åˆ†ç±»å™¨çš„å®ç°åœ¨`LinearRegressionClassifier.train/classify` in `classifiers.py`ã€‚

```
Doing classification
--------------------
data:           digits
classifier:             lr
training set size:      5000
Extracting features...
Training...
Validating...
807 correct out of 1000 (80.7%).
Testing...
779 correct out of 1000 (77.9%).
```

è¿™ä¸ªåˆ†ç±»å™¨çš„ç›®æ ‡å‡½æ•°ä¸º$\text{min}\|Y-W^TX\|_F^2+\frac{\lambda}{2}\|W\|_F^2$

è§£æè§£ä¸ºï¼š$\text{W}=(XX^T+\lambda I)^{-1}XY^T$

å¯¹æ¯ä¸ªç‰¹å¾å‘é‡xï¼Œæˆ‘ä»¬æœ‰è¯„ä¼°å‡½æ•°ï¼š$\operatorname{score}(x,y)=\|y-W^Tx\|^2$

MNISTæ•°æ®é›†ï¼šæ¯ä¸ªæ•°æ®ä¸º28\*28=784ç°åº¦æ•°å­—å›¾åƒï¼Œå¹¶ä¸”æ‰€æœ‰åŠŸèƒ½å’Œæ ‡ç­¾éƒ½æ˜¯numpyæ ¼å¼ã€‚

## Question 1: K-Means clustering (4 points)

K-Meansç®—æ³•ç®€å•è¿‡ç¨‹å¦‚ä¸‹ï¼š

Initialize: $\mu_j\leftarrow\text{Random}(\boldsymbol{x}_i),j=1,...,k$

Repeat until convergence: ï¼ˆé¦–å…ˆå®šä¹‰ç‰¹å¾$x,y$ä¹‹é—´çš„è·ç¦»ï¼š$\operatorname{dist}(x,y)=\|x-y\|^2$ï¼‰

- Compute cluster assignment (labels): $y_i=h(\boldsymbol{x}_i)=\underset{j}{\operatorname{argmin}}\left\|\boldsymbol{\mu}_j-\boldsymbol{x}_i\right\|_2^2,i=1,...,n$
- Compute means: $\mu_j\longleftarrow\mathrm{Mean}(\{\boldsymbol{x}_i|y_i=j\}),j=1,...,k$

<img src="report_learning.assets/image-20230508171532480.png" alt="image-20230508171532480" style="zoom:67%;" />

---

åœ¨å®Œæˆä½œä¸šçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘å‘ç°å……åˆ†äº†è§£æ•°æ®å˜é‡çš„shapeéå¸¸é‡è¦ï¼Œæ—¢æœ‰åŠ©äºå¸®åŠ©æˆ‘ä»¬ç†è§£æ•°æ®çš„æ„ä¹‰ï¼Œä¹Ÿèƒ½å¤Ÿé¿å…è¿›è¡Œç»´åº¦é”™è¯¯çš„è®¡ç®—ã€‚æ•°æ®å˜é‡çš„shapeæƒ…å†µå¦‚ä¸‹ï¼š

- `trainingData`: n x dimï¼Œç”¨äºè®­ç»ƒçš„æ•°æ®ä¸€å…±nä¸ªï¼Œæ¯ä¸ªæ•°æ®ä¸ºdimå…ƒï¼›
- `cluster_no`: n x 1ï¼Œè¡¨ç¤ºå½“å‰æ¯ä¸ªè®­ç»ƒæ•°æ®å¯¹åº”çš„ç°‡æ ‡å·ï¼›
- `self.clusters`: k x dimï¼Œå¯¹åº”kä¸ªç°‡ä¸­å¿ƒã€‚

ç®—æ³•çš„å®ç°ä¸»è¦æ˜¯åˆ©ç”¨Numpyåº“ä¸­çš„å‡½æ•°æ¥å®Œæˆï¼Œå…¶ä¸­è®¡ç®—ç‰¹å¾ä¹‹é—´çš„distæ¯”è¾ƒå›°éš¾ã€‚ç”±äº$\mu_j$ä¸$x_i$äºŒè€…ç»´åº¦ä¸åŒï¼Œå¯ä»¥å°†`trainingData` reshape ä¸º(n, 1, dim)ï¼Œè€Œå°†`self.clusters` reshape ä¸º(1, k, dim)ï¼ŒäºŒè€…ç›¸å‡å¾—åˆ°ç»´åº¦ä¸º(n, k, dim)çš„å·®å‘é‡ï¼Œè¯¥å·®å‘é‡åªéœ€è¦åœ¨`ord = 2, axis = 2çš„`æ¡ä»¶ä¸‹æ±‚èŒƒæ•°å³å¯å¾—åˆ°ç‰¹å¾ä¹‹é—´çš„distã€‚

## Question 2: KNN classifier (3 points)

K-Nearest-Neighbors (KNN)ç®—æ³•åŸç†ä¸º: Find ğ‘˜ nearest neighbors of ğ’™. Label ğ’™ with the majority label within the ğ‘˜ nearest neighbors. å…¶ä¸­ï¼Œç‰¹å¾$x,y$ä¹‹é—´çš„è·ç¦»ï¼š$\operatorname{dist}(x,y)=\|x-y\|^2$

<img src="report_learning.assets/image-20230508171502817.png" alt="image-20230508171502817" style="zoom:67%;" />

---

æ•°æ®å˜é‡çš„shapeæƒ…å†µå¦‚ä¸‹ï¼š

- ` self.trainingData`: training(5000)  x dim(784)ï¼Œç”¨äºè®­ç»ƒçš„æ•°æ®ä¸€å…±5000ä¸ªï¼Œæ¯ä¸ªæ•°æ®ä¸º784å…ƒã€‚
- `data`: training(5000)  x dim(784)ï¼Œè¡¨ç¤ºéªŒè¯é›†çš„æ•°æ®ï¼Œä¸€å…±æœ‰5000ä¸ªï¼Œæ¯ä¸ªæ•°æ®ä¸º784å…ƒï¼ˆå³åƒç´ ä¸ªæ•°ï¼‰
- `dist`:  validation(1000) x training(5000)ï¼Œ`dist[i][j]`å°±ä»£è¡¨è·ç¦»`(validationData[i] - trainingData[j])^2`

ä½†åœ¨è¿™é‡Œä½¿ç”¨question1çš„æ–¹æ³•è®¡ç®—distï¼Œä¼šå¼€è¾Ÿå‡ºä¸€ä¸ªå¤§å°ä¸º(1000, 5000, 786)floatæ•°ç»„ï¼Œè¿™ä¼šé€ æˆ**å†…å­˜é—®é¢˜**ã€‚å› æ­¤æˆ‘ä»¬åªèƒ½é‡‡ç”¨å¾ªç¯ï¼Œæ¯æ¬¡å¯¹ä¸€ä¸ªéªŒè¯é›†æ•°æ®åˆ°æ‰€æœ‰è®­ç»ƒæ•°æ®çš„distï¼Œç„¶åæ ¹æ®knnç®—æ³•ç®—å‡ºè¯¥éªŒè¯é›†æ•°æ®çš„é¢„æµ‹labelï¼Œæœ€åè¿”å›ä¸€ä¸ªå¤§å°ä¸º1000çš„labelæ•°ç»„ã€‚

run `python dataClassifier.py â€“c knn â€“n 5`

è¯¥å‘½ä»¤è°ƒç”¨KNN Classifierï¼Œè¿è¡Œç»“æœå¦‚ä¸‹ï¼Œå‡†ç¡®ç‡ä¸º91.0%. 

```
data:           digits
classifier:             knn
training set size:      5000
Extracting features...
Training...
Validating...
917 correct out of 1000 (91.7%).
Testing...
910 correct out of 1000 (91.0%).
```

## Question 3: Perceptron (or Softmax Regression) (4 points)

<img src="report_learning.assets/image-20230508171718365.png" alt="image-20230508171718365" style="zoom:50%;" />

Perceptronç®—æ³•å¦‚ä¸‹ï¼š

Test perceptron algorithm with5000 training data, 1000 validation data and 1000 test data.

Perceptron
$$
\begin{array}{c}t=f(x)=g\left(W^Tx+b\right)=[f_1(x),...,f_i(x)]^T\\ f_i(x)=g_i(w_i^Tx+b_i)\end{array}
$$
The output of ğ‘“(ğ‘¥) can be regarded as the following multinomial distribution:
$$
p(y=i|x)=f_i(x)=\frac{e^{w_i^T x+b_i}}{\sum_{j=1}^l e^{w_j^T x+b_j}}
$$
Weight and bias is updated as follows:
$$
w_j^{(t+1)}=w_j^{(t)}-\eta\lambda w_j^{(t)}-\frac{\eta}{k}\sum_{i=1}^k\nabla_{w_j}-\log p(y=y_i|x_i)|_{w_j=w_j^{(t)}} \\

 b_j^{(t+1)}=b_j^{(t)}-n\lambda b_j^{(t)}-\frac{\eta}{k}\sum_{i=1}^k\nabla_{b_j}-\log p(y=y_i|x_i)|_{y_j=a_j^{(t)}}
$$
where:
$$
\nabla_{w_j}-\log p(y=y_i|x_i)=\left\{\begin{matrix}p(y=j|x_i)x_i,j\neq y_i\\ (p(y=j|x_i)-1)x_i,j=y_i\end{matrix}\right. \\

\nabla_{b_j}-\log p(y=y_i|x_i)=\left\{\begin{matrix}p(y=j|x_i),j\neq y_i\\ p(y=j|x_i)-1,j=y_i\end{matrix}\right.
$$
æ ¹æ®ä¸Šè¿°å…¬å¼è®¡ç®—$p(y=j|x_i)$ï¼Œä»¥åŠæ›´æ–°æƒé‡$w$ å’Œåç½®$b$ å³å¯ã€‚

---

run `python dataClassifier.py â€“c perceptron`

è¯¥å‘½ä»¤è°ƒç”¨Perceptron Classifierï¼Œè¿è¡Œç»“æœå¦‚ä¸‹ï¼Œå‡†ç¡®ç‡ä¸º87.7%. 

```
data:           digits
classifier:             perceptron
training set size:      5000
Extracting features...
Training...
Starting iteration  0 ...
Starting iteration  10 ...
Starting iteration  20 ...
Starting iteration  30 ...
Starting iteration  40 ...
Validating...
902 correct out of 1000 (90.2%).
Testing...
877 correct out of 1000 (87.7%).
```

Which following images is most likely represent the weights learned by the perception?

(a) å¯ä»¥å‘ç°a ä¸­å¯¹åº”çš„æ˜¯ä¸€ä¸ªâ€ç›¸å¯¹æ¨¡ç³Šâ€œçš„æ‰‹å†™æ•°å­—ï¼Œè€Œb ä¸­å¯¹åº”çš„æ˜¯æ¯”è¾ƒæ¸…æ™°çš„æ‰‹å†™æ•°å­—ï¼Œå¯¹äºä¸€ä¸ªè®­ç»ƒåçš„æ¨¡å‹ï¼Œæƒé‡å›¾å®é™…ä¸Šè¡¨ç¤ºçš„æ˜¯æ¯ä¸ªè¾“å…¥åƒç´ å¯¹äºè¾“å‡ºç±»åˆ«çš„é‡è¦ç¨‹åº¦ï¼Œè®­ç»ƒçš„æ¨¡å‹éœ€è¦è¯†åˆ«ä¸åŒäººå†™å‡ºçš„ä¸åŒæ ·å­çš„æ•°å­—ï¼Œå› æ­¤æƒé‡å›¾ä¸å¯èƒ½å¦‚åŒb ä¸€æ ·è¿™ä¹ˆæ¸…æ™°ã€‚b å¯èƒ½å¯¹åº”çš„æ˜¯åˆšå¼€å§‹è®­ç»ƒæ—¶çš„æƒé‡å›¾ï¼Œè€Œa åˆ™å¯¹åº”è®­ç»ƒç»“æŸåçš„æƒé‡å›¾ã€‚å®é™…æƒé‡å›¾å¦‚ä¸‹ï¼š

![weights](report_learning.assets/weights.png)

## Question 4: SVM with sklearn (2 points)

<img src="report_learning.assets/image-20230508171745563.png" alt="image-20230508171745563" style="zoom:50%;" />

SVMç®—æ³•ï¼šåŸºäºè®­ç»ƒé›†Dåœ¨æ ·æœ¬ç©ºé—´ä¸­æ‰¾åˆ°ä¸€ä¸ªåˆ’åˆ†è¶…å¹³é¢ï¼Œå°†ä¸åŒç±»åˆ«çš„æ ·æœ¬åˆ†å¼€ã€‚è¿™ä¸ªè¶…å¹³é¢éœ€è¦æ»¡è¶³é—´éš”æœ€å¤§è¿™ä¸ªæ¡ä»¶ã€‚
$$
\begin{array}{c}\max_{w,b} \text{margin}(w,b)\\ \text{s.t.}y_i(w-x_i+b)\geq1,1\leq i\leq n\end{array}
$$
ç­‰ä»·äºï¼š
$$
\min_{w,b}\frac{1}{2}{||w||^2_2}\\
s.t. y_i(wx_i+b) \ge 1, 1\le i\le n\\
$$
ç”¨æ‹‰æ ¼æœ—æ—¥æ³•æ±‚è§£å…¶å¯¹å¶é—®é¢˜ï¼Œæ‹‰æ ¼æœ—æ—¥å‡½æ•°ä¸ºï¼š
$$
L\bigl(\boldsymbol{w},b,\boldsymbol{\alpha}\bigr)=\frac{1}{2}\left\|\boldsymbol{w}\right\|^2+\sum_{i=1}^{m}\alpha_i\left(1-y_i\left(\boldsymbol{w}^T\boldsymbol{x}_i+b\right)\right)
$$
ä»¤å…¶åå¯¼æ•°ä¸ºé›¶ï¼Œå¾—åˆ°ä¸¤ä¸ªå…³ç³»å¼ï¼Œå¸¦å…¥åŸå¼æœ‰ï¼š
$$
\max&\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_j y_i y_j\boldsymbol{x}_i^T\boldsymbol{x}_j\\ s.t.&\sum_{i=1}^m\alpha_i y_i=\boldsymbol{0},\alpha_i\geq0
$$
ä¹Ÿå°±æ˜¯ä»»åŠ¡æŒ‡å¯¼ä¹¦ä¸­ç»™å®šçš„ä¼˜åŒ–é—®é¢˜ï¼š
$$
\min_\alpha\frac{1}{2}\alpha^TA\alpha-\mathbf{1}^T\alpha\\ \text{s.t.}\begin{matrix}y^T\alpha=0\\ 0\leq\alpha_i\leq C\end{matrix}
$$

---

implement a SVM algorithm with the following hyperparameters:

$C=5$

kernel type: ${RBF}(K(x,y)=\exp-\frac{(x-y)^2}{2\sigma^2})\quad\text{with}\ \sigma=10$

åœ¨sklearnçš„SVMå‡½æ•°ä¸­ï¼Œå…³é”®å‡½æ•°å°±æ˜¯` sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)`

é¢˜ç›®ä¸æ³¨é‡Šä¸­è¦æ±‚è®¾ç½®å››ä¸ªå‚æ•°ï¼Œå…¶ä¸­ç›´æ¥è®¾ç½®`c=5.0`ï¼Œ`kernel='rbf'`ï¼Œ`decision_function_shape='ovr'`ï¼Œå¯¹äºRBFå‡½æ•°ï¼Œ`K(x, x') = exp(-gamma * ||x - x'||^2)`ï¼Œå› æ­¤`gamma`å‚æ•°ç›´æ¥è®¾ç½®ä¸º`gamma=1/(2*(10.0)**2)`ã€‚

The relationship between the gamma parameter in the API and Ïƒ in original RBF
$$
\text{gamma}=\frac{1}{2\sigma^2}
$$

---

run `python dataClassifier.py â€“c svm`ï¼Œæœ€ç»ˆæ­£ç¡®ç‡ä¸º93.1%

```
Doing classification
--------------------
data:           digits
classifier:             svm
training set size:      5000
Extracting features...
Training...
Validating...
944 correct out of 1000 (94.4%).
Testing...
931 correct out of 1000 (93.1%).
```


## Question 5: Better Classification Accuracy (2 points + 1 bonus)

èµ·åˆé‡‡ç”¨SVMæ¨¡å‹ï¼Œå¹¶ä¸”åå¤æ”¹è¿›è¶…å‚æ•°åè®­ç»ƒå‡ºæ¥çš„æœ€ç»ˆæ­£ç¡®ç‡ä¸º94.8%ï¼Œäºæ˜¯å†³å®šé‡‡ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç»è¿‡åå¤æ”¹è¿›è¶…å‚æ•°å¯ä»¥è¾¾åˆ°95.6%çš„æ­£ç¡®ç‡ã€‚

æ•´ä½“åŸºäºå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMultilayer Perceptronï¼ŒMLPï¼‰æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networkï¼‰æ¨¡å‹ã€‚ç»è¿‡å°è¯•å‘ç°è®¾ç½®2ä¸ªéšè—å±‚ï¼Œæ¯å±‚å‡ä¸º256ä¸ªç¥ç»å…ƒï¼ŒåŒæ—¶`batch_size`è®¾ç½®ä¸º16ã€‚

run `python dataClassifier.py â€“c best`ï¼Œæœ€ç»ˆæ­£ç¡®ç‡ä¸º95.6%

```
Doing classification
--------------------
data:           digits
classifier:             best
training set size:      5000
Extracting features...
Training...
Validating...
Testing...
956 correct out of 1000 (95.6%).
```

